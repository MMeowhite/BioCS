{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPStXK1kRBeYKdSakX3H873"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WXTaTvIsYVg_","executionInfo":{"status":"ok","timestamp":1741252365421,"user_tz":-480,"elapsed":9716,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"b0b912a4-3985-47c4-f8c1-8dd74d9cf4fa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))"]},"metadata":{},"execution_count":1}],"source":["import torch\n","\n","x = torch.tensor([3.0])\n","y = torch.tensor([2.0])\n","\n","x + y, x * y, x / y, x ** y"]},{"cell_type":"code","source":["x = torch.arange(4)\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3HSlkg35Yv1F","executionInfo":{"status":"ok","timestamp":1741179596055,"user_tz":-480,"elapsed":12,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"927cf8e7-7a8c-48bb-e5ec-418de908e326"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["x[3], x[0:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j06uTxiqY2ky","executionInfo":{"status":"ok","timestamp":1741179608660,"user_tz":-480,"elapsed":13,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"999a911f-8551-4877-a7b7-0150e4575e7b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(3), tensor([0, 1, 2]))"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["len(x), x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZLKacOINZHJs","executionInfo":{"status":"ok","timestamp":1741179642223,"user_tz":-480,"elapsed":4,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"7c10591d-49b3-4d2f-8bdb-9223bf45395d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, torch.Size([4]))"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["A = torch.arange(20).reshape(5, 4)\n","A, A.T"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VurQD8wCZKM-","executionInfo":{"status":"ok","timestamp":1741179678625,"user_tz":-480,"elapsed":19,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"d226243e-c24f-49b3-c8d1-2be9fd330f90"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0,  1,  2,  3],\n","         [ 4,  5,  6,  7],\n","         [ 8,  9, 10, 11],\n","         [12, 13, 14, 15],\n","         [16, 17, 18, 19]]),\n"," tensor([[ 0,  4,  8, 12, 16],\n","         [ 1,  5,  9, 13, 17],\n","         [ 2,  6, 10, 14, 18],\n","         [ 3,  7, 11, 15, 19]]))"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["B = torch.tensor([\n","    [1, 2, 3],\n","    [2, 0, 4],\n","    [3, 4, 5]\n","])\n","B, B==B.T"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMnJB-X_Zbxc","executionInfo":{"status":"ok","timestamp":1741179732116,"user_tz":-480,"elapsed":13,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"8c737eee-4bad-4518-ab98-591f66956338"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1, 2, 3],\n","         [2, 0, 4],\n","         [3, 4, 5]]),\n"," tensor([[True, True, True],\n","         [True, True, True],\n","         [True, True, True]]))"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["A vector is a generalization of a scalar, and a matrix is a generalization of a vector."],"metadata":{"id":"h5HpSJS-u04N"}},{"cell_type":"code","source":["X = torch.arange(24).reshape(2, 3, 4)\n","X"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1UclqjH6un2n","executionInfo":{"status":"ok","timestamp":1741252381270,"user_tz":-480,"elapsed":74,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"369ca037-b063-4d7d-b929-85695936f596"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0,  1,  2,  3],\n","         [ 4,  5,  6,  7],\n","         [ 8,  9, 10, 11]],\n","\n","        [[12, 13, 14, 15],\n","         [16, 17, 18, 19],\n","         [20, 21, 22, 23]]])"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["Two tensors of the same shape will always produce a tensor of the same shape when any element-wise operation is applied."],"metadata":{"id":"PdeABcimvm_I"}},{"cell_type":"code","source":["A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n","B = A.clone() # allocate the new memory to store variable B\n","id(B) == id(A), A, A + B"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ademTPtivT4s","executionInfo":{"status":"ok","timestamp":1741252716845,"user_tz":-480,"elapsed":17,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"3dd62da6-be9c-4624-9b3e-9b87db1210bb"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(False,\n"," tensor([[ 0.,  1.,  2.,  3.],\n","         [ 4.,  5.,  6.,  7.],\n","         [ 8.,  9., 10., 11.],\n","         [12., 13., 14., 15.],\n","         [16., 17., 18., 19.]]),\n"," tensor([[ 0.,  2.,  4.,  6.],\n","         [ 8., 10., 12., 14.],\n","         [16., 18., 20., 22.],\n","         [24., 26., 28., 30.],\n","         [32., 34., 36., 38.]]))"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## Hadamard Product of Two Matrices\n","\n","The element-wise multiplication of two matrices is called the **Hadamard Product**, also known as the **element-wise product** or **pointwise product**. The Hadamard Product is computed by multiplying the corresponding elements of two matrices to produce a new matrix of the same shape.\n","\n","### Definition\n","Given two matrices **A** and **B** of the same shape (m×n), their Hadamard Product **C** is also an m×n matrix, where each element \\( c_{ij} \\) is the product of the corresponding elements \\( a_{ij} \\) and \\( b_{ij} \\):\n","\n","\\[ C = A \\circ B \\]\n","\\[ c_{ij} = a_{ij} \\cdot b_{ij} \\]\n","\n","### Example\n","Suppose we have two 2×2 matrices **A** and **B**:\n","\n","\\[ A = \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} \\]\n","\\[ B = \\begin{pmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{pmatrix} \\]\n","\n","Their Hadamard Product **C** is:\n","\n","\\[ C = \\begin{pmatrix} a_{11} \\cdot b_{11} & a_{12} \\cdot b_{12} \\\\ a_{21} \\cdot b_{21} & a_{22} \\cdot b_{22} \\end{pmatrix} \\]\n","\n","### Applications\n","The Hadamard Product is widely used in various fields, including mathematics, engineering, image processing, and machine learning."],"metadata":{"id":"UWW9QvHZwcbq"}},{"cell_type":"code","source":["A * B"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cp9R2rlzwyDs","executionInfo":{"status":"ok","timestamp":1741252935403,"user_tz":-480,"elapsed":9,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"4a9ed289-42d1-46c0-d576-bd26db3ff5c5"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  0.,   1.,   4.,   9.],\n","        [ 16.,  25.,  36.,  49.],\n","        [ 64.,  81., 100., 121.],\n","        [144., 169., 196., 225.],\n","        [256., 289., 324., 361.]])"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["a = 2\n","X = torch.arange(24).reshape(2, 3, 4)\n","a + X, (a * X).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5T1Rl6Otw1du","executionInfo":{"status":"ok","timestamp":1741252967337,"user_tz":-480,"elapsed":17,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"c5fed409-b756-43d6-a9b8-297bdfeb1688"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[ 2,  3,  4,  5],\n","          [ 6,  7,  8,  9],\n","          [10, 11, 12, 13]],\n"," \n","         [[14, 15, 16, 17],\n","          [18, 19, 20, 21],\n","          [22, 23, 24, 25]]]),\n"," torch.Size([2, 3, 4]))"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Use `sum()` function to compute the sum the all elements in the matrix"],"metadata":{"id":"CKOG4jfixCEO"}},{"cell_type":"code","source":["X = torch.arange(10).reshape(2, 5)\n","X.sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"necPgGZXxIOd","executionInfo":{"status":"ok","timestamp":1741253044452,"user_tz":-480,"elapsed":49,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"dada18b4-a53a-4561-81a0-c7f32e2c413e"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(45)"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Additionally, we can use `axis=` to indicate the sum of given axis."],"metadata":{"id":"Tt1kvGXvxlPb"}},{"cell_type":"code","source":["X_sum_axis0 = X.sum(axis=0)\n","X_sum_axis1 = X.sum(axis=1)\n","X, X_sum_axis0, X_sum_axis1, X.shape, X_sum_axis0.shape, X_sum_axis1.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6FBA_Mf9xvwc","executionInfo":{"status":"ok","timestamp":1741253277494,"user_tz":-480,"elapsed":9,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"755cc217-6d26-44c4-d00d-0ed4cc72e682"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0, 1, 2, 3, 4],\n","         [5, 6, 7, 8, 9]]),\n"," tensor([ 5,  7,  9, 11, 13]),\n"," tensor([10, 35]),\n"," torch.Size([2, 5]),\n"," torch.Size([5]),\n"," torch.Size([2]))"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["X_sum = X.sum(axis=[0, 1]) # same as .sum()\n","X_sum"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PDhU9mhyLAM","executionInfo":{"status":"ok","timestamp":1741253317580,"user_tz":-480,"elapsed":8,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"ad2f84b1-e0d8-4638-bb89-8e9f05e93ef9"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(45)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["The same effects is done with `.mean()`"],"metadata":{"id":"j8_mEvN0ye_P"}},{"cell_type":"code","source":["X = torch.arange(10, dtype=torch.float32).reshape(2, 5)\n","X, X.mean(), X.mean(axis=0), X.mean(axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"km68lYomymNL","executionInfo":{"status":"ok","timestamp":1741253503849,"user_tz":-480,"elapsed":5,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"d511a7ae-8e66-4116-83f8-6d932d91ac50"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0., 1., 2., 3., 4.],\n","         [5., 6., 7., 8., 9.]]),\n"," tensor(4.5000),\n"," tensor([2.5000, 3.5000, 4.5000, 5.5000, 6.5000]),\n"," tensor([2., 7.]))"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["A.mean() == A.sum() / A.numel(), A.mean(axis=0) == A.sum(axis=0) / A.shape[0], A.mean(axis=1) == A.sum(axis=1) / A.shape[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAaWFoucyzyJ","executionInfo":{"status":"ok","timestamp":1741253582523,"user_tz":-480,"elapsed":16,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"5fafb5be-5842-4c65-e83f-df781606cf22"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(True),\n"," tensor([True, True, True, True]),\n"," tensor([True, True, True, True, True]))"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["In the example above, the dimension will be lost when performing the summation. Of course, it is also possible to keep the axis unchanged when calculating the total sum or the mean with parameter `keepdim=True`."],"metadata":{"id":"rUI1VdJgzqz4"}},{"cell_type":"code","source":["sum_X = X.sum(axis=1, keepdim=True)\n","X, sum_X, X / sum_X # boardcasting mechanism was performed by the same dimension with the two matrix."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yd2dxD0LzrtA","executionInfo":{"status":"ok","timestamp":1741253834347,"user_tz":-480,"elapsed":8,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"4adfc95e-b034-4552-c131-38d54a77a262"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0., 1., 2., 3., 4.],\n","         [5., 6., 7., 8., 9.]]),\n"," tensor([[10.],\n","         [35.]]),\n"," tensor([[0.0000, 0.1000, 0.2000, 0.3000, 0.4000],\n","         [0.1429, 0.1714, 0.2000, 0.2286, 0.2571]]))"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["Cumulative Sum: `cumsum()`"],"metadata":{"id":"CzkH529W0pLU"}},{"cell_type":"code","source":["X = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n","\n","cum_sum_X_dim0 = X.cumsum(dim=0)\n","\n","cum_sum_X_dim1 = X.cumsum(dim=1)\n","\n","print(\"Original Tensor X:\")\n","print(X)\n","print(\"\\nCumulative Sum along dim=0 (columns):\")\n","print(cum_sum_X_dim0)\n","print(\"\\nCumulative Sum along dim=1 (rows):\")\n","print(cum_sum_X_dim1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMr_Snp60vUh","executionInfo":{"status":"ok","timestamp":1741254131179,"user_tz":-480,"elapsed":45,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"104d27d3-40cb-4b39-b47b-c79517f6b956"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Tensor X:\n","tensor([[1., 2., 3.],\n","        [4., 5., 6.]])\n","\n","Cumulative Sum along dim=0 (columns):\n","tensor([[1., 2., 3.],\n","        [5., 7., 9.]])\n","\n","Cumulative Sum along dim=1 (rows):\n","tensor([[ 1.,  3.,  6.],\n","        [ 4.,  9., 15.]])\n"]}]},{"cell_type":"markdown","source":["Dot product: The **dot product**, also known as the **scalar product** or **inner product**, is an operation between two vectors that results in a scalar value.\n","Matrice product: The"],"metadata":{"id":"TW15j1lE1fPu"}},{"cell_type":"code","source":["x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n","y = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n","z = torch.tensor([[1,2], [3,4], [5,6]], dtype=torch.float32)\n","x, y, x.shape, y.shape, torch.mul(x, y), torch.matmul(x, z), torch.mm(x, z) # mul为点积， mm,matmul为矩阵乘法"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yr9YlxTI5OWS","executionInfo":{"status":"ok","timestamp":1741257333397,"user_tz":-480,"elapsed":15,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"78e1ae0b-908b-46a2-d131-4471a291c8a4"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[1., 2., 3.],\n","         [4., 5., 6.]]),\n"," tensor([[1., 2., 3.],\n","         [4., 5., 6.]]),\n"," torch.Size([2, 3]),\n"," torch.Size([2, 3]),\n"," tensor([[ 1.,  4.,  9.],\n","         [16., 25., 36.]]),\n"," tensor([[22., 28.],\n","         [49., 64.]]),\n"," tensor([[22., 28.],\n","         [49., 64.]]))"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":[],"metadata":{"id":"t4TXLr6tBo7Z"}},{"cell_type":"code","source":["u = torch.tensor([3.0, -4.0])\n","torch.norm(u), torch.abs(u).sum(), torch.norm(torch.ones(4, 9))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D2Epq2DpBqQQ","executionInfo":{"status":"ok","timestamp":1741257443370,"user_tz":-480,"elapsed":10,"user":{"displayName":"Peng Miao","userId":"15092246065165496906"}},"outputId":"8804ec63-e3c4-4ad8-f556-11c227634aa1"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(5.), tensor(7.), tensor(6.))"]},"metadata":{},"execution_count":44}]}]}